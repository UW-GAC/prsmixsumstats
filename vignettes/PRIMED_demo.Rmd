---
title: "Calculating summary statistics for elastic net"
author: "Dan Schaid"
date: "`r Sys.Date()`"
output: html_document
vignette: >
  %\VignetteIndexEntry{Calculating summary statistics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```


Create test data of 3 groups with PRS (nprs) and covariates (sex, age, cov1, cov2)


```{r}
library(prsmixsumstats)
```

```{r}
## simulate training data
nprs <- 1000

n1 <- 1000
n2 <- 500
n3 <- 50

dat1 <- sim_test_dat(n1, nprs, prev=.1, beta.sd=2)
dat2 <- sim_test_dat(n2, nprs, prev=.1, beta.sd=2)
dat3 <- sim_test_dat(n3, nprs, prev=.1, beta.sd=2)

## make some y's missing
dat1$y[3:20] <- NA
dat2$y[15:30] <- NA
dat3$y[1:16] <- NA

## make some x's missing
dat1$x[3:5, 4:5] <- NA
dat2$x[20:30, 5:8] <- NA
dat3$x[2:10, 6:9] <- NA
```

# Biobanks Create Summary Statistics (3 demo biobanks)

```{r}
sumstats <- list()
sumstats[[1]] <-  make_sumstats(dat1$x, dat1$y)
sumstats[[2]] <-  make_sumstats(dat2$x, dat2$y)
sumstats[[3]] <-  make_sumstats(dat3$x, dat3$y)
```

# CC: sum over biobanks

```{r}
total_stats <- combine_sumstats(sumstats)
xx <- total_stats$sumstats$xx
xy <- total_stats$sumstats$xy

yvar <- total_stats$yvar
ysum <- attr(total_stats$sumstats, "ysum")
nobs <- attr(total_stats$sumstats, "nobs")

## vmat used later for simulations to create simulated validation data
## used to choose lambda that gives best fit
## after standardizing x and y, vmat is a correlation matrix of y and x
vmat <- make_varxy(xx, xy, yvar)

## note that there is no intercept because x and y are centered on their means

penalty_factor <- rep(1,ncol(xx))
## if don't penalize adjusting covariates: should they be penalized??
## penalty_factor[1:4] <- 0
```

# fit a grid of lambda's

```{r}
## alpha weights abs(beta) and (1-alpha) weights beta^2
alpha <- 0.5
lambda_max <- max(abs(xy))/alpha
lambda_frac <- seq(from=1, to=.05, by= -.05)
lambda_vec <- lambda_max * lambda_frac
nfit <- length(lambda_vec)

# suggest setting this to 500
# for vignette, limit number of iterations for time
maxiter <- 50

beta_init <- rep(0, ncol(xx))
fit_sumstats <- list()

cat("================ lambda.frac = ", lambda_frac[1], " ==================\n")

fit_sumstats[[1]] <-  glmnet_sumstats(total_stats$sumstats, beta_init, alpha=alpha, lambda=lambda_vec[1], 
                                penalty_factor, maxiter=maxiter, tol=1e-7,f.adj=2.0, verbose=FALSE)

#for vignette, limit number of iterations to 3
#fit <- 3
for(j in 2:nfit){
  ptm <- proc.time()
  cat("================ lambda.frac = ", lambda_frac[j], " ==================\n")
  ## use warm start for next fit
  beta_init <- fit_sumstats[[j-1]]$beta
  fit_sumstats[[j]] <-  glmnet_sumstats(total_stats$sumstats, beta_init, alpha=alpha, lambda=lambda_vec[j], 
                                    penalty_factor, maxiter=maxiter, tol=1e-7,f.adj=32.0, verbose=FALSE)
  print(proc.time() - ptm)
}
```


# approximate AUC and R2 for case/control

```{r}
ncase <- ysum
ncont <- nobs - ncase

ncase; ncont
auc <- nbeta <- r2 <- rep(0, nfit)
for(i in 1:nfit){
  tmp <- auc_glmnet_sumstats(fit_sumstats[[i]]$beta, xx, yvar, ncase, ncont)
  auc[i] <- tmp$auc
  r2[i]  <- tmp$r2
  nbeta[i] <- sum( abs(fit_sumstats[[i]]$beta*penalty_factor) > 1e-6)
} 

plot(nbeta, r2)
plot(lambda_frac, auc)
plot(lambda_frac, nbeta)
max(auc)

loss <- rep(0, nfit)

for (i in 1:nfit) {
  beta <- as.vector(fit_sumstats[[i]]$beta)
  loss[i] <- yvar- 2*t(beta) %*% xy +  t(beta) %*% xx %*% beta
}

plot(lambda_frac, loss)
plot(nbeta, loss)
index <- (1:nfit)[loss == min(loss)]
beta <- as.vector(fit_sumstats[[index]]$beta)
hist(beta)
nbeta[index]
```



# choose lambda penalty

now choose best lambda based on simulated data, using
the covariance matrix of Y with X to simulate validation data

not sure if this is best, or consider alternative with cross-validation
with setup from make_sumstats to create xval data sets: need to discuss

```{r}
set.seed(123)

# only 2 simulations for vignette
#nsim <- 5
nsim <- 2
nfit <- length(fit_sumstats)

## compute correlation of predicted using beta from training data 
## with observed validation data (simulated)
## see equation (12) p 472 of Mak et al.
cormat <- lossmat <- matrix(0, nrow = nsim, ncol = nfit)
cr <- rep(0, nfit)
loss <- rep(0, nfit)

for(isim in 1:nsim) {
  yx <- simulate_yx(nobs, vmat)
  y.sim <- yx[, 1]
  x.sim <- yx[, -1]
   
  ## center y
  y.c <- y.sim - mean(y.sim)
  ## center and scale x's
  x.c <- scale(x.sim) / sqrt(nobs - 1)
  
  for (i in 1:nfit) {
    beta <- as.vector(fit_sumstats[[i]]$beta)
    xb <- (x.c %*% beta)
    num <- t(xb) %*% y.c
    den <- as.vector(sqrt((t(xb) %*% xb) * t(y.c) %*% y.c))
    if(abs(den) < 1e-6){
      cr[i] <- 0
    } else{
      cr[i] <- as.numeric(num / den)
    }
    loss[i] <- sum( (y.sim - xb)^2)
  }
  
  cormat[isim, ] <- cr
  lossmat[isim,] <- loss
}


cor_mean <- apply(cormat, 2, mean)
loss_mean <- apply(lossmat, 2, mean)
plot(lambda_vec, loss_mean)
plot(lambda_vec, cor_mean)

index_best <- (1:nfit)[loss_mean == min(loss_mean)][1]
index_best
loss_mean
lambda_vec[index_best]

## now look at results from best fit
beta <- as.vector(fit_sumstats[[index_best]]$beta)
hist(beta)
range(beta)
table(abs(beta) > 1e-6)
auc[index_best]
plot(rep(lambda_vec, nsim), as.vector(lossmat))
```
