#' Make summary statistics
#' 
#' From input design matrix x and outcome vector y, compute x'x and x'y
#'
#' important: x and y must have the same number of subjects
#' and ordered the same way according to subject ID, but ID 
#' should not be a column of X
#' 
#' returns a sumstats object, which contains two list elements:
#' 1. matrix of x'x where x is nxp design matrix
#' 2. vector of x'y where y is nx1 trait vector
#' attributes of the sumstats object: nsubj, nmiss, nobs, colsum, ysum, yssq
#'
#' @param x design matrix with rows as individual observations
#' @param y vector with outcomes, length should match nrow(x)
#' @param center boolean for whether to center and scale x and y
#' @return sumstats object with elements xx and xy 
#' @export
make_sumstats <- function(x, y, center=TRUE){
  ## important: x and y must have the same number of subjects
  ## and ordered the same way according to subject ID, but ID
  ## should not be a column of X

  ## create sum stats xx and xy
  xcol.names <-  colnames(x)
  names.miss <- xcol.names == "" | is.null(xcol.names)
  if(any(names.miss)){
    stop("col names of x are missing")
  }

  nsubj <- nrow(x)
  is.miss.x <- rowSums(is.na(x)) > 0
  is.miss.y <-  is.na(y)
  is.miss <- is.miss.x | is.miss.y
  nmiss <- sum(is.miss)
  nobs <- nsubj - nmiss

  x <- x[!is.miss, , drop=FALSE]
  csum <- colSums(x)
  if (center) {
    x <- scale(x, scale=FALSE)
  }
  #xx <- (t(x) %*% x)
  xx <- crossprod(x)
  rownames(xx) <- xcol.names
  colnames(xx) <- xcol.names

  y <- y[!is.miss]
  ysum <- sum(y, na.rm=TRUE)
  if (center) {
    y <- scale(y, scale=FALSE)
  }
  #xy <- t(x) %*% y
  xy <- crossprod(x, y)
  #names(xy) <-  xcol.names

  yssq <- sum(y^2, na.rm=TRUE)
  
  ss <- new_sumstats(xx, xy, nsubj, nmiss, nobs, csum, ysum, yssq, centered=center)

  return(ss)
}


#' Compute elastic net from summary statistics
#' @description
#' Fit an elastic net model based on summary statistics for specified
#' penalty parameters alpha and lambda
#'
#' @param sumstats object with elements xx: matrix of x'x where x is nxp design matrix, xy: vector of x'y where y is nx1 trait vector
#' @param beta_init is px1 vector of starting values
#' @param alpha (range 0-1) is fraction of penalty for L1 penalty
#' and (1-alpha) is fraction for L2 penalty
#' @param lambda is penalty parameter
#' @param penalty_factor is px1 vector for weighting penalties of design matrix columns.
#' Typically used as value of 0 for terms not to be penalized and 1 for terms to penalize
#' @param maxiter is maximum number of cyclic coordinate descent iterations.
#' One iteration is over all p parameters.
#' @param tol is tolerance to check convergence
#' @param f.adj (f.adj >=1) is an adjustment factor to guard against diverging betas, which can
#' occur when n is small relative to p. The default value of 2 should work most of the time.
#' If beta parameters are found to diverge, larger values can be used.
#' Note that larger values of f.adj cause more iterations.
#' See Yang, Y., & Zou, H. (2012). A coordinate majorization descent algorithm for L1 penalized learning.
#' Journal of Statistical Computation and Simulation, 84(1), 84â€“95. https://doi.org/10.1080/00949655.2012.695374
#' @param verbose prints summary results at each iteration if verbose=TRUE
#' @details
#' Cyclic coordinate descent is used to fit an elastic net model based on minimizing penalized least squared.
#' @returns list with fitted beta, number of iterations, convergence (true/false), and input penalty parameters
#' alpha and lambda
#' @author Dan Schaid (schaid@mayo.edu)
#' @export
glmnet_sumstats <- function(sumstats, beta_init, alpha, lambda, penalty_factor,
                               maxiter=500, tol=1e-5, max_backtrack=10, verbose=FALSE){
  ##validate_sumstats(sumstats)
  xx <- sumstats$xx
  xy <- sumstats$xy
  
  update_beta <- function(index, beta_curr, loss_curr) {
    ## single coordinate update with line search
    # compute numerator and denominator
    numer <- xy[index] - xx[index, ] %*% beta_curr + xx[index,index] * beta_curr[index]
    denom <- xx[index,index] + lambda_pen[index] * (1 - alpha)
    
    
    # propose update using soft-thresholding
    beta_candidate <- soft(numer, alpha * lambda_pen[index]) / denom
    
    # line search: try step sizes 1, 1/2, 1/4, ...
    step <- 1.0
    
    
    for (k in 1:max_backtrack) {
      
      beta_trial <- (1 - step) * beta_curr[index] + step * beta_candidate
      loss_trial <- update_loss(loss_old, beta_curr, index, beta_trial, 
                                xx, xy, alpha, lambda_pen)
      
      if (loss_trial <= loss_curr) {   # accept if loss decreases
        beta_curr[index] <- beta_trial
        loss_curr <- loss_trial
        break
      }
      
      step <- step / 2  # backtrack
    }
    
    list(beta = beta_curr, loss = loss_curr)
  }
  compute_loss <- function(beta, xx, xy, alpha,lambda_pen){
    loss <-  sum( t(beta) %*% xx %*% beta - 2*t(beta) %*% xy) + alpha*sum(lambda_pen * abs(beta)) +
      ((1-alpha)/2)*sum(lambda_pen*beta^2)
    return(loss)
  }
  update_loss <- function(loss_old, beta, j, new_val, xx, xy, alpha, lambda_pen) {
    old_val <- beta[j]
    delta   <- new_val - old_val
    
    # --- Frobenius part ---
    xb <- sum(xx[i, ] * beta )
    dQ   <- 2 * delta * (xb - xy[j]) + delta^2 * xx[j, j]
    
    pen_l1_delta <- alpha * lambda_pen[j] * (abs(new_val) - abs(old_val))
    pen_l2_delta <- ((1 - alpha)/2) * lambda_pen[j] * (new_val^2 - old_val^2)
     
    loss_new <- loss_old + (dQ + pen_l1_delta + pen_l2_delta)

    return(loss_new)
  }
  
  np <- ncol(xx)
  if(is.null(penalty_factor)) penalty_factor <- rep(1, np)
  lambda_pen <- lambda * penalty_factor
  if(is.null(beta_init)){
    beta_init <- rep(0, np)
  }
  
  stopifnot(length(beta_init) == ncol(xx))
  stopifnot(length(penalty_factor) == ncol(xx))
  
  converge <- FALSE
  beta_curr <- beta_init
  loss_curr <-  compute_loss(beta_curr, xx, xy, alpha,lambda_pen)
  

  ## iterate over active set
  for(iter in 1:maxiter){
   
    ## KKT test for zero entries
    grad <- abs(xy - xx %*% beta_curr)
    active <- grad > alpha * lambda_pen
  
    loss_old <- loss_curr
    beta_old  <- beta_curr
    
    for(j in 1:np){
      if(!active[j]) next
      res <- update_beta(j, beta_curr, loss_curr)
      beta_curr <- res$beta
      loss_curr <- res$loss
    }
    
    
    loss_rel_change <- abs(loss_curr - loss_old) / (abs(loss_old) + tol)
    beta_change <- max(abs(beta_curr - beta_old))
    
    if (verbose) {
      cat("active set: iter =", iter,
          ", loss_curr =", loss_curr, "loss_rel_change = ", loss_rel_change,
          ", beta range =", range(beta_curr), "\n")
    }
    if (loss_rel_change < tol || beta_change < tol) {
      converge <- TRUE
      break
    }
  }
  
  
  return(list(beta=beta_curr, loss=loss_curr, iter=iter, converge=converge, alpha=alpha, lambda=lambda))
}

soft <- function(x, gamma){
  if(x > gamma){
    s <- x - gamma
  } else if (x < -gamma){
    s <- x + gamma
  }else{
    s <- 0
  }
  return(s)
}


#' calculate AUC from glmnet_sumstats results
#' @param beta beta from glmnet_sumstats
#' @param xx X'X matrix that went into glmnet_sumstats
#' @param vary variance of Y
#' @param ncase number of cases
#' @param ncont number of controls
#' @return list with AUC and R2
#' @importFrom stats pnorm
#' @export
auc_glmnet_sumstats <- function(beta, xx, vary, ncase, ncont){
  ssr <- t(beta) %*% xx %*% beta
  sst <- vary #* nsubj
  r2 <- ssr/sst
  a <- (ncase + ncont)^2/(ncase * ncont)
  d <- sqrt(a*r2)/sqrt(1-r2)
  auc <- pnorm(d/sqrt(2))
  return(list(auc=auc, r2=r2))
}


#' Simulate example data
#' @param nsubj number of subjects
#' @param nprs number of PRS
#' @param prev prev
#' @param beta.sd beta.sd
#' @param seed seed
#' @importFrom stats rbinom rnorm runif
#' @export
sim_test_dat <- function(nsubj, nprs, prev=.1, beta.sd=2, seed=42){
  set.seed(seed)
 ## large beta.sd allows larger beta's
  sex <-  rbinom(nsubj,size=1,prob=.5)
  age <- round(runif(nsubj, min=40, max=70),0)
  cov1 <- rnorm(nsubj)
  cov2 <- rnorm(nsubj)
  x <- cbind(age, sex, cov1, cov2, matrix(rnorm(nsubj*nprs), nrow=nsubj))
  beta <- rnorm(nprs+4, sd=beta.sd)
  xb <- x %*% beta + log(prev/(1-prev))
  p <- exp(xb) / (1 + exp(xb))
  y <- as.vector(1*(runif(nsubj) <= p))
  colnames(x) <- c("age","sex","cov1","cov2", paste0("PRS00", 1:nprs))
  return(list(y=y, x=x))
}



metrics_sumstats <- function(sumstats, fit_grid){
  ncase <- attr(sumstats, "ysum")
  ncont <-  attr(sumstats, "nobs") - ncase
  nalpha <- nrow(fit_grid)
  nlambda <- ncol(fit_grid)
  vary <- sumstats$vary
  auc <- nbeta <- loss <- matrix(0, nalpha, nlambda)
  for(i in 1:nalpha){
    for(j in 1:nlambda){
      beta <- as.vector(fit_grid[[i,j]]$beta)
      tmp <- auc_glmnet_sumstats(beta, sumstats$xx, vary, ncase, ncont)
      auc[i,j] <- tmp$auc
      loss[i,j] <- vary - 2*t(beta) %*% sumstats$xy +  t(beta) %*% sumstats$xx %*% beta
      nbeta[i,j] <- sum( abs(beta) > 1e-6)
    }
  }
  return(list(auc=auc, loss=loss, nbeta=nbeta))
}
